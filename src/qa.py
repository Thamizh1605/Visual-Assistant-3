# src/qa.py
"""
qa.py
- Given: image_description (string) and user_question (string)
- Return: answer string generated by a local LLM (GPT4All) if available, otherwise simple rule-based answer.
"""

from typing import Optional
import os
from gpt4all import GPT4All  # assuming gpt4all is installed

# Path to your local GPT4All model
MODEL_FILE = "Meta-Llama-3-8B-Instruct.Q4_0.gguf"
MODEL_PATH = "/Users/thamizharasan/Library/Application Support/nomic.ai/GPT4All"

_gpt_handle = None

def ask_question(image_description: str, question: str, max_tokens=256) -> str:
    """
    Uses local LLM to answer the question using the image_description as context.
    If no local LLM available, use a simple template-based reply.
    """
    context_prompt = (
        "You are an assistant for a visually impaired user. Based on the following "
        "image description, answer the user question concisely.\n\n"
        f"Image description: {image_description}\n\n"
        f"User question: {question}\n\n"
        "Answer:"
    )

    try:
        global _gpt_handle
        if _gpt_handle is None:
            model_file_candidate = os.path.join(MODEL_PATH, MODEL_FILE)
            if not os.path.exists(model_file_candidate):
                raise FileNotFoundError(f"Model file not found: {model_file_candidate}")
            _gpt_handle = GPT4All(model_file_candidate, model_path=MODEL_PATH)

        resp = _gpt_handle.generate(context_prompt, max_tokens=max_tokens)
        return resp

    except Exception as e:
        print("[qa] gpt4all generation error:", e)

    # Fallback response (rule-based)
    q = question.lower()
    if "anyone" in q or "someone" in q or "any person" in q:
        if "man" in image_description or "person" in image_description or "someone" in image_description:
            return "Yes, I see a person in the scene."
        else:
            return "I do not detect a person based on the image description."

    return f"Based on the image: {image_description}. Regarding your question: I cannot run a local LLM here; please install GPT4All and place a gguf model in {MODEL_PATH} to enable full answers."


print(ask_question("A man is standing below the tree","What is the man doing?"))